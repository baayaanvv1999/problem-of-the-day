# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19MDy8RhZXtZWzEqNoGR3b_E9kAvp4qdp
"""

!pip install requests
!pip install beautifulsoup4

import time
import csv
import requests
from bs4 import BeautifulSoup

# Bayan Sulaiman Alkhalili

webpage= 'https://www.linkedin.com/jobs/search?keywords=AI&location=Oman&geoId=103619019&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'

file = open('linkedin-jobs.csv', 'a')
writer = csv.writer(file)
writer.writerow(['Title', 'Company', 'Location', 'Apply'])

def linkedin_scraper(webpage, start=0):
  if start> 0:
    next_page = webpage + '&start={}'.format(start)
  else:
    next_page = webpage
  print(str(next_page))
  response = requests.get(str(next_page))
  soup = BeautifulSoup(response.content,'html.parser')

  jobs = soup.find_all('div', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')
  for job in jobs:
    job_title = job.find('h3', class_='base-search-card__title').text.strip()
    job_company = job.find('h4', class_='base-search-card__subtitle').text.strip()
    job_location = job.find('span', class_='job-search-card__location').text.strip()
    job_link = job.find('a', class_='base-card__full-link')['href']

    writer.writerow([
    job_title.encode('utf-8'),
    job_company.encode('utf-8'),
    job_location.encode('utf-8'),
    job_link.encode('utf-8')
    ])

  print(job_title)
  print('Data updated')

for i in range(5):
  linkedin_scraper(webpage, i*25)
  # be polite with the website to avoid banning
  time.sleep(3)
file.close()
print('File closed')